{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dedde0c0",
   "metadata": {},
   "source": [
    "# Post-Training Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a491f06",
   "metadata": {},
   "source": [
    "Let's start with importing some libraries that we will need for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a928cec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/research/lib/python3.10/site-packages/torchvision/__init__.py:9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/research/lib/python3.10/site-packages/torchvision/extension.py:6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internally_replaced_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _get_extension_path\n\u001b[1;32m      9\u001b[0m _HAS_OPS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_has_ops\u001b[39m():\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/research/lib/python3.10/site-packages/torchvision/_internally_replaced_utils.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimportlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmachinery\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _get_torch_home\n\u001b[1;32m      7\u001b[0m _HOME \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvision\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m _USE_SHARDED_DATASETS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.hub'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchao.quantization import quantize_, Int8WeightOnlyConfig\n",
    "from torchao.utils import get_model_size_in_bytes\n",
    "import argparse\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d6a73",
   "metadata": {},
   "source": [
    "If you're running into issues importing these libraries, check which python env the jupyter kernel is using!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9389eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d1b2f",
   "metadata": {},
   "source": [
    "### Formatting the Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b91d25f",
   "metadata": {},
   "source": [
    "We will start by normalizing our MNIST dataset with the pre-calculated mean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2866ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    train=True, transform=transform\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    train=False, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847df14e",
   "metadata": {},
   "source": [
    "Then we'll load it into these data loaders for easy access to loading the dataset when the time comes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca54541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3811d339",
   "metadata": {},
   "source": [
    "### Neural Network Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e76b35",
   "metadata": {},
   "source": [
    "Let's define our neural network class now! We'll keep it super simple with 2 layers. Since each image is 28x28 pixels, we will have our input be of dimension 28x28. We want to output which digit the image is and since there are 10 different possible digits, we will use an output of dimension 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63d8686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(28*28, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d160640",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c00e6b",
   "metadata": {},
   "source": [
    "Now that we have defined our model, let's train it using the dataset and see what the type and format of the weights are. (Expecting 32-bit floating point numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaff205",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7d8163",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32 = Network()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_fp32.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de7a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model_fp32.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model_fp32(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb34ea4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model_fp32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac9afe5",
   "metadata": {},
   "source": [
    "Now, lets quantize the model to INT8 and see what happens to the weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aaf98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_int8 = copy.deepcopy(model_fp32)\n",
    "quantize_(model_int8, Int8WeightOnlyConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2759ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d58e63",
   "metadata": {},
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edf74e0",
   "metadata": {},
   "source": [
    "So far, we have trained our model using 32-bit floating point weights/biases and created a quantized version using INT8. Let's see what the accuracy change is between the two and how much the model size has changed as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368af5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32.eval()\n",
    "\n",
    "fp32_correct = 0\n",
    "fp32_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model_fp32(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        fp32_total += target.size(0)\n",
    "        fp32_correct += (predicted == target).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dda91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Accuracy FP32: {100 * fp32_correct / fp32_total:.2f}%\")\n",
    "print(f\"FP32 SIZE: {get_model_size_in_bytes(model_fp32) / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e6fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_int8.eval()\n",
    "\n",
    "int8_correct = 0\n",
    "int8_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model_int8(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        int8_total += target.size(0)\n",
    "        int8_correct += (predicted == target).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0723098",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Accuracy INT8: {100 * int8_correct / int8_total:.2f}%\")\n",
    "print(f\"INT8 SIZE: {get_model_size_in_bytes(model_int8) / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498a1eac",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c966785",
   "metadata": {},
   "source": [
    "From the comparison, we see that the accuracy reduced slightly but the model size has decreased heavily. This is due to the fact that we have reduced the precision from FP32 to INT8 which is around a reduction of 4x in bytes. Our model sizes reflect this accurately. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
